{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/zheng22/miniconda3/envs/hf_gpu_dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def create_factorized_compression_for_linear(source_linear, rank):\n",
    "  \"\"\"\n",
    "  Adapt from: https://github.com/cloneofsimo/lora/blob/master/lora_diffusion/cli_svd.py\n",
    "  Create a factorized compression for a given linear layer using SVD.\n",
    "  Args:\n",
    "      source_linear (nn.Linear): The original linear layer to be compressed.\n",
    "      rank (int, optional): The rank for the factorization. If None, it will be calculated based on rank_factor.\n",
    "      rank_factor (float, optional): The factor to determine the rank if rank is not provided. Default is 0.3.\n",
    "  Returns:\n",
    "      nn.Sequential: A sequential container of the compressed linear layers.\n",
    "  \"\"\"\n",
    "  with torch.no_grad():\n",
    "      dtype = source_linear.weight.dtype\n",
    "      # Check if the source linear layer has a bias term\n",
    "      if hasattr(source_linear, 'bias'):\n",
    "          bias = source_linear.bias\n",
    "      else:\n",
    "          bias = None\n",
    "      # Calculate the total number of parameters in the source linear layer\n",
    "      source_num_params = sum(param.numel() for param in source_linear.parameters())\n",
    "      # Get the weight matrix of the source linear layer\n",
    "      source_linear_weight = source_linear.weight.data\n",
    "      # Ensure rank is less than the minimum dimension of the weight matrix\n",
    "      assert rank < min(source_linear_weight.shape)\n",
    "      # Perform SVD on the weight matrix\n",
    "      U, S, Vh = torch.linalg.svd(source_linear_weight.float())\n",
    "      # Truncate U, S, Vh to the specified rank\n",
    "      U = U[:, :rank]  # Shape: [out_features, rank]\n",
    "      S = S[:rank]     # Shape: [rank]\n",
    "      Vh = Vh[:rank, :]  # Shape: [rank, in_features]\n",
    "      # Incorporate singular values into U\n",
    "      U = U @ torch.diag(S)  # Shape: [out_features, rank]\n",
    "      # Flatten U and Vh for quantile computation\n",
    "      U_flatten = U.flatten()\n",
    "      Vh_flatten = Vh.flatten()\n",
    "      # Define the maximum quantization size\n",
    "      max_quant_size = 2**23\n",
    "      # Compute high and low quantile values for clamping\n",
    "      if len(U_flatten) + len(Vh_flatten) >= max_quant_size:\n",
    "          dist2 = U_flatten[:min(len(U_flatten), max_quant_size)]\n",
    "          dist3 = Vh_flatten[:min(len(Vh_flatten), max_quant_size)]\n",
    "          hi_val = max(torch.quantile(dist3, 1), torch.quantile(dist2, 1))\n",
    "      else:\n",
    "          dist = torch.cat([U_flatten, Vh_flatten])\n",
    "          hi_val = torch.quantile(dist, 1)\n",
    "      low_val = -hi_val\n",
    "      # Clamp U and Vh to the quantile values\n",
    "      U = U.clamp(low_val, hi_val)\n",
    "      Vh = Vh.clamp(low_val, hi_val)\n",
    "      # Create the down projection linear layer (Vh)\n",
    "      lora_down = nn.Linear(Vh.shape[1], Vh.shape[0], dtype=dtype, bias=False, device=source_linear_weight.device)\n",
    "      lora_down.weight.data = Vh.to(device=source_linear_weight.device, dtype=dtype)\n",
    "      # Create the up projection linear layer (U)\n",
    "      lora_up = nn.Linear(U.shape[1], U.shape[0], dtype=dtype, bias=bias is not None, device=source_linear_weight.device)\n",
    "      lora_up.weight.data = U.to(device=source_linear_weight.device, dtype=dtype)\n",
    "      # If the original linear layer had a bias, copy it to the up projection layer\n",
    "      if bias is not None:\n",
    "          lora_up.bias = nn.Parameter(bias.clone())\n",
    "      # Print compression ratio (for debugging purposes)\n",
    "      #print('compression', sum(param.numel() for param in ret.parameters()) / source_num_params)\n",
    "      return lora_down, lora_up\n",
    "    \n",
    "class AdaVocabHead(nn.Module):\n",
    "  def __init__(self, lm_head, sub_vocab_dim, dora=False, svd=False, activation_func=None):\n",
    "    self.dora = dora\n",
    "    hidden_size, vocab_size = lm_head.in_features, lm_head.out_features\n",
    "    super().__init__()\n",
    "    if svd: # SVD initialization\n",
    "      self.A, self.B = create_factorized_compression_for_linear(lm_head, sub_vocab_dim)\n",
    "      if dora: \n",
    "        self.m = nn.Parameter(lm_head.weight.T.norm(p=2, dim=1, keepdim=True))  # (hidden_size, 1)\n",
    "    else:  # Random initialization\n",
    "      self.A = nn.Linear(hidden_size, sub_vocab_dim, bias=False)\n",
    "      self.B = nn.Linear(sub_vocab_dim, vocab_size, bias=False)\n",
    "      std_dev = 1 / math.sqrt(sub_vocab_dim)\n",
    "      nn.init.normal_(self.A.weight, 0, std_dev)\n",
    "      nn.init.zeros_(self.B.weight)\n",
    "    self.activation_func = activation_func\n",
    "    \n",
    "  def forward(self, x):\n",
    "    # x.shape: (..., hidden_size), A.shape: (hidden_size, sub_vocab_dim), B.shape: (sub_vocab_dim, vocab_size)\n",
    "    if self.dora:\n",
    "      comb_weight = self.A.weight.T @ self.B.weight.T  # (hidden_size, vocab_size)\n",
    "      norm_vec = comb_weight.norm(p=2, dim=1, keepdim=True)  # (hidden_size, 1)\n",
    "      directional_component = comb_weight / norm_vec  # (hidden_size, vocab_size)\n",
    "      dora_weight = self.m * directional_component  # (hidden_size, vocab_size)\n",
    "      ada_vocab_logits = x @ dora_weight  # ada_vocab_logits.shape: (..., vocab_size)\n",
    "    else:\n",
    "      logits = self.A(x)\n",
    "      if self.activation_func is not None:\n",
    "          logits = self.activation_func(logits)\n",
    "      ada_vocab_logits = self.B(logits)  # ada_vocab_logits.shape: (..., vocab_size)  \n",
    "    return ada_vocab_logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:25<00:00, 12.82s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"/home/z/zheng22/AdaVocab/experiment_ckpts/gemma-2b_SFT-2024-06-10-123619/checkpoint-11592\", torch_dtype=torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(2, 2048, dtype=torch.bfloat16).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████| 2/2 [00:17<00:00,  8.99s/it]\n",
      "`config.hidden_act` is ignored, you should use `config.hidden_activation` instead.\n",
      "Gemma's activation function will be set to `gelu_pytorch_tanh`. Please, use\n",
      "`config.hidden_activation` if you want to override this behaviour.\n",
      "See https://github.com/huggingface/transformers/pull/29402 for more details.\n",
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.43s/it]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (20x256000 and 2048x20)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2048\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[1;32m      4\u001b[0m new_adahead \u001b[38;5;241m=\u001b[39m AdaVocabHead(model\u001b[38;5;241m.\u001b[39mlm_head, \u001b[38;5;241m20\u001b[39m, dora\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, svd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m----> 5\u001b[0m \u001b[43mnew_adahead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_gpu_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/hf_gpu_dev/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 88\u001b[0m, in \u001b[0;36mAdaVocabHead.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m     86\u001b[0m   \u001b[38;5;66;03m# x.shape: (..., hidden_size), A.shape: (hidden_size, sub_vocab_dim), B.shape: (sub_vocab_dim, vocab_size)\u001b[39;00m\n\u001b[1;32m     87\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdora:\n\u001b[0;32m---> 88\u001b[0m     comb_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m@\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m  \u001b[38;5;66;03m# (hidden_size, vocab_size)\u001b[39;00m\n\u001b[1;32m     89\u001b[0m     norm_vec \u001b[38;5;241m=\u001b[39m comb_weight\u001b[38;5;241m.\u001b[39mnorm(p\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# (1, vocab_size)\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     directional_component \u001b[38;5;241m=\u001b[39m comb_weight \u001b[38;5;241m/\u001b[39m norm_vec  \u001b[38;5;66;03m# (hidden_size, vocab_size)\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (20x256000 and 2048x20)"
     ]
    }
   ],
   "source": [
    "new_adahead = AdaVocabHead(model.lm_head, 20, dora=True, svd=True).cuda()\n",
    "new_adahead(x).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Linear(in_features=20, out_features=256000, bias=False),\n",
       " Linear(in_features=2048, out_features=20, bias=False))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_adahead.A, new_adahead.B, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AdaVocabHead(\n",
       "  (A): Linear(in_features=20, out_features=256000, bias=False)\n",
       "  (B): Linear(in_features=2048, out_features=20, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_adahead.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "comb_weight = new_adahead.B.weight.T @ new_adahead.A.weight.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_vec = comb_weight.norm(p=2, dim=1, keepdim=True)  # (hidden_size, 1)\n",
    "directional_component = comb_weight / norm_vec  # (hidden_size, vocab_size)\n",
    "dora_weight = new_adahead.m * directional_component  # (hidden_size, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_adahead.m = nn.Parameter(model.lm_head.weight.T.norm(p=2, dim=1, keepdim=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2048, 1])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_adahead.m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/z/zheng22/miniconda3/envs/hf_gpu_dev/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(2048, 256000, dtype=torch.float32)\n",
    "b = torch.randn(2048, dtype=torch.float32)\n",
    "c = torch.randn(2048, 2048, dtype=torch.float32)\n",
    "torch.save((a, b, c), \"test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.34s/it]\n"
     ]
    }
   ],
   "source": [
    "model_path = \"/home/z/zheng22/AdaVocab/experiment_ckpts/gemma-2b_SFT-2024-06-10-123619/checkpoint-11592\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_linear_weight = model.lm_head.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, Vh = torch.linalg.svd(source_linear_weight.float())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256000, 256000]) torch.Size([2048]) torch.Size([2048, 2048])\n",
      "<class 'torch.Tensor'> <class 'torch.Tensor'> <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "print(U.shape, S.shape, Vh.shape)\n",
    "print(type(U), type(S), type(Vh))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_rank = 2048\n",
    "U = U[:, :max_rank].clone()  # Shape: [out_features, rank]\n",
    "S = S[:max_rank].clone()     # Shape: [rank]\n",
    "Vh = Vh[:max_rank, :].clone()  # Shape: [rank, in_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([256000, 2048]) torch.Size([2048]) torch.Size([2048, 2048])\n"
     ]
    }
   ],
   "source": [
    "print(U.shape, S.shape, Vh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save((U, S, Vh), \"svd.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf_gpu_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
